{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Old Code.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"0LkoMg9ba6wN","colab_type":"code","colab":{}},"source":["# custom dataset that fetches image pairs from a given scene\n","# DEPRECATED, use h5Dataset for performance improvements\n","class ImagePairDataset(Dataset):\n","    def __init__(self, rgb_path, depth_path, transform=None):\n","        '''\n","        rgb_path: path to actual rgb photos (.../rgb)\n","        depth_path: path to actual depth photos (.../depth)\n","        transform: transforms to perform. at minimum, must convert to tensor\n","        '''\n","        # lists of full path to every image\n","        self.rgb_paths = [rgb_path + f for f in listdir(rgb_path)]\n","        self.depth_paths = [depth_path + f for f in listdir(depth_path)]\n","\n","        # rgb vs depth sizes might mismatch, so take min\n","        size = min(len(self.rgb_paths), len(self.depth_paths))\n","\n","        # truncate the lists so every image has a pair, also discard last image\n","        # because it may be corrupted\n","        self.rgb_paths = self.rgb_paths[:size-2]\n","        self.depth_paths = self.depth_paths[:size-2]\n","\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","\n","        #rgb = np.moveaxis(np.array(Image.open(self.rgb_paths[index])), 0, -1)\n","\n","        # open images and convert to numpy, scale depth down\n","        rgb = np.array(Image.open(self.rgb_paths[index]))\n","        depth = np.array(Image.open(self.depth_paths[index])) / 65536\n","\n","        # try to circumvent some strange size errors with a few pictures,\n","        # if the occur just return previous index's picture pair\n","        try:\n","            while rgb.shape[2] != 3:\n","                index -= 1\n","                rgb = np.array(Image.open(self.rgb_paths[index]))\n","                depth = np.array(Image.open(self.depth_paths[index])) / 65536\n","        except:\n","            index -= 1\n","            rgb = np.array(Image.open(self.rgb_paths[index]))\n","            depth = np.array(Image.open(self.depth_paths[index])) / 65536\n","\n","        if self.transform:\n","            rgb = self.transform(rgb)\n","            depth = self.transform(depth).squeeze()\n","            \n","        return rgb, depth\n","    \n","    def __len__(self):\n","        return min(len(self.rgb_paths), len(self.depth_paths))\n","\n","\n","\n","class Baseline1(nn.Module):\n","    def __init__(self):\n","        super(Baseline1, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(3, 20, kernel_size=11, padding=5),\n","            nn.Conv2d(20, 40, kernel_size=11, padding=5),\n","            nn.Conv2d(40, 20, kernel_size=7, padding=3),\n","            nn.Conv2d(20, 1, kernel_size=7, padding=3)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Model1(nn.Module):\n","    def __init__(self):\n","        super(Model1, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                                                    #in: 640x480\n","            nn.Conv2d(3, 40, kernel_size=11, padding=5, stride=2), #out: 320x240\n","            nn.Conv2d(40, 80, kernel_size=7, padding=3, stride=2), #out: 160x120\n","            nn.Conv2d(80, 160, kernel_size=5, padding=2, stride=2), #out: 80x60\n","            nn.Conv2d(160, 320, kernel_size=3, padding=1, stride=2),#out: 40x30\n","        )\n","\n","        self.trans_conv = nn.Sequential(\n","                                                                    #in: 40x30\n","            nn.ConvTranspose2d(320, 160, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 80x60\n","            nn.ConvTranspose2d(160, 80, kernel_size=5, padding=2, \n","                               stride=2, output_padding=1),        #out: 160x120\n","            nn.ConvTranspose2d(80, 40, kernel_size=7, padding=3, \n","                               stride=2, output_padding=1),        #out: 320x240\n","            nn.ConvTranspose2d(40, 1, kernel_size=11, padding=5, \n","                               stride=2, output_padding=1),        #out: 640x480\n","        )\n","\n","    def forward(self, x):\n","        return self.trans_conv(self.conv(x))\n","\n","class Model2(nn.Module):\n","    def __init__(self):\n","        super(Model2, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                                                    #in: 640x480\n","            nn.Conv2d(3, 40, kernel_size=21, padding=10, stride=2), #out: 320x240\n","            nn.SELU(),\n","            nn.Conv2d(40, 80, kernel_size=11, padding=5, stride=2), #out: 160x120\n","            nn.SELU(),\n","            nn.Conv2d(80, 160, kernel_size=7, padding=3, stride=2), #out: 80x60\n","            nn.SELU(),\n","            nn.Conv2d(160, 320, kernel_size=3, padding=1, stride=2),#out: 40x30\n","        )\n","\n","        self.trans_conv = nn.Sequential(\n","                                                                    #in: 40x30\n","            nn.ConvTranspose2d(320, 160, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 80x60\n","            nn.SELU(),\n","            nn.ConvTranspose2d(160, 80, kernel_size=7, padding=3, \n","                               stride=2, output_padding=1),        #out: 160x120\n","            nn.SELU(),\n","            nn.ConvTranspose2d(80, 40, kernel_size=11, padding=5, \n","                               stride=2, output_padding=1),        #out: 320x240\n","            nn.SELU(),\n","            nn.ConvTranspose2d(40, 1, kernel_size=21, padding=10, \n","                               stride=2, output_padding=1),        #out: 640x480\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.trans_conv(self.conv(x))\n","\n","class Model3(nn.Module):\n","    def __init__(self):\n","        super(Model3, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                                                      #in: 640x480\n","            nn.Conv2d(3, 100, kernel_size=5, padding=2, stride=2),   #out: 320x240\n","            nn.SELU(),\n","            nn.Conv2d(100, 200, kernel_size=3, padding=1, stride=2), #out: 160x120\n","            nn.SELU(),\n","            nn.Conv2d(200, 500, kernel_size=3, padding=1, stride=2), #out: 80x60\n","        )\n","\n","        self.trans_conv = nn.Sequential(\n","                                                                    #in: 80x60\n","            nn.ConvTranspose2d(500, 200, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 160x120\n","            nn.SELU(),\n","            nn.ConvTranspose2d(200, 100, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 320x240\n","            nn.SELU(),\n","            nn.ConvTranspose2d(100, 1, kernel_size=5, padding=2, \n","                               stride=2, output_padding=1),        #out: 640x480\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.trans_conv(self.conv(x))\n","\n","class Model4(nn.Module):\n","    def __init__(self):\n","        super(Model4, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                                                    #in: 640x480\n","            nn.Conv2d(3, 40, kernel_size=51, padding=25, stride=2), #out: 320x240\n","            nn.SELU(),\n","            nn.Conv2d(40, 80, kernel_size=21, padding=10, stride=2), #out: 160x120\n","            nn.SELU(),\n","            nn.Conv2d(80, 160, kernel_size=11, padding=5, stride=2), #out: 80x60\n","            nn.SELU(),\n","            nn.Conv2d(160, 320, kernel_size=3, padding=1, stride=2),#out: 40x30\n","        )\n","\n","        self.trans_conv = nn.Sequential(\n","                                                                    #in: 40x30\n","            nn.ConvTranspose2d(320, 160, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 80x60\n","            nn.SELU(),\n","            nn.ConvTranspose2d(160, 80, kernel_size=11, padding=5, \n","                               stride=2, output_padding=1),        #out: 160x120\n","            nn.SELU(),\n","            nn.ConvTranspose2d(80, 40, kernel_size=21, padding=10, \n","                               stride=2, output_padding=1),        #out: 320x240\n","            nn.SELU(),\n","            nn.ConvTranspose2d(40, 1, kernel_size=51, padding=25, \n","                               stride=2, output_padding=1),        #out: 640x480\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.trans_conv(self.conv(x))\n","\n","class Model5(nn.Module):\n","    def __init__(self):\n","        super(Model5, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                                                    #in: 640x480\n","            nn.BatchNorm2d(3),\n","            nn.Conv2d(3, 40, kernel_size=51, padding=25, stride=2), #out: 320x240\n","            nn.SELU(),\n","            nn.BatchNorm2d(40),\n","            nn.Conv2d(40, 80, kernel_size=21, padding=10, stride=2), #out: 160x120\n","            nn.SELU(),\n","            nn.BatchNorm2d(80),\n","            nn.Conv2d(80, 160, kernel_size=11, padding=5, stride=2), #out: 80x60\n","            nn.SELU(),\n","            nn.BatchNorm2d(160),\n","            nn.Conv2d(160, 320, kernel_size=3, padding=1, stride=2),#out: 40x30\n","            nn.BatchNorm2d(320)\n","        )\n","\n","        self.trans_conv = nn.Sequential(\n","                                                                    #in: 40x30\n","            nn.ConvTranspose2d(320, 160, kernel_size=3, padding=1, \n","                               stride=2, output_padding=1),        #out: 80x60\n","            nn.SELU(),\n","            nn.BatchNorm2d(160),\n","            nn.ConvTranspose2d(160, 80, kernel_size=11, padding=5, \n","                               stride=2, output_padding=1),        #out: 160x120\n","            nn.SELU(),\n","            nn.BatchNorm2d(80),\n","            nn.ConvTranspose2d(80, 40, kernel_size=21, padding=10, \n","                               stride=2, output_padding=1),        #out: 320x240\n","            nn.SELU(),\n","            nn.BatchNorm2d(40),\n","            nn.ConvTranspose2d(40, 1, kernel_size=51, padding=25, \n","                               stride=2, output_padding=1),        #out: 640x480\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.trans_conv(self.conv(x))\n","\n","\n","# DEPRECATED\n","def load_dataset_older(root_path, batch_size=16, ratios=(.7,.15,.15)):\n","    sub_datasets = []\n","    for scene in listdir(root_path):\n","        sub_datasets.append(\n","            ImagePairDataset(\n","                rgb_path = root_path + scene + \"/rgb/\",\n","                depth_path = root_path + scene + \"/depth/\",\n","                transform = transforms.ToTensor()\n","            )\n","        )\n","\n","    dataset = ConcatDataset(sub_datasets)\n","    num_samples = len(dataset)\n","    len1 = int(num_samples*ratios[0])\n","    len2 = int(num_samples*ratios[1])\n","    train_data, val_data, test_data = random_split(dataset,\n","                                    (len1, len2, num_samples-len1-len2))\n","    \n","    print(len(train_data), len(val_data), len(test_data))\n","    \n","    train_loader = DataLoader(train_data, batch_size=batch_size, \n","                              shuffle=False, num_workers=0)\n","    val_loader = DataLoader(val_data, batch_size=batch_size, \n","                              shuffle=False, num_workers=0)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, \n","                              shuffle=False, num_workers=0)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# returns train, val, test loaders when you provide a path to the data\n","# DEPRECATED\n","def load_dataset_old(root_path, batch_size=16, num_folders=(6, 2, 2)):\n","    '''\n","    root_path: path to data folder, which must contain scene folders inside.\n","        for example, provide path to \".../Data\" for this structure:\n","            /Data\n","                /computer_room\n","                    /rgb\n","                    /depth\n","                /dentist_office\n","                    /rgb\n","                    /depth\n","    batch_size: used by data loaders\n","    num_folders: of the x number of scene folders (we have 10 right now), \n","        list how many folders should go to train, val, and test respectively\n","    '''\n","\n","    train_sub_datasets = []\n","    val_sub_datasets = []\n","    test_sub_datasets = []\n","\n","    for i, scene in enumerate(listdir(root_path)):\n","\n","        if \".\" in scene:\n","            pass\n","\n","        if i < num_folders[0]:\n","            train_sub_datasets.append(\n","                ImagePairDataset(\n","                    rgb_path = root_path + scene + \"/rgb/\",\n","                    depth_path = root_path + scene + \"/depth/\",\n","                    transform = transforms.ToTensor()\n","                )\n","            )\n","        \n","        elif i < num_folders[0] + num_folders[1]:\n","            val_sub_datasets.append(\n","                ImagePairDataset(\n","                    rgb_path = root_path + scene + \"/rgb/\",\n","                    depth_path = root_path + scene + \"/depth/\",\n","                    transform = transforms.ToTensor()\n","                )\n","            )\n","\n","        else:\n","            test_sub_datasets.append(\n","                ImagePairDataset(\n","                    rgb_path = root_path + scene + \"/rgb/\",\n","                    depth_path = root_path + scene + \"/depth/\",\n","                    transform = transforms.ToTensor()\n","                )\n","            )\n","        \n","    train_data = ConcatDataset(train_sub_datasets)\n","    val_data = ConcatDataset(val_sub_datasets)\n","    test_data = ConcatDataset(test_sub_datasets)\n","\n","    train_loader = DataLoader(train_data, batch_size=batch_size, \n","                              shuffle=True, num_workers=0)\n","    val_loader = DataLoader(val_data, batch_size=batch_size, \n","                              shuffle=True, num_workers=0)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, \n","                              shuffle=True, num_workers=0)\n","    \n","    return train_loader, val_loader, test_loader\n","\n","def mem_train(model, batch_size=16, batch_multiplier = 1, learning_rate=0.01, \n","              num_epochs=1, ratios=(.7, .15, .15)):\n","\n","    plotlosses = PlotLosses()\n","\n","    print(\"loading data...\")\n","    train_loader, val_loader = load_dataset(\n","        file_path=darie_path,\n","        batch_size=batch_size,\n","        ratios=ratios\n","        )[:2]\n","    print(\"data loading complete\")\n","\n","    torch.manual_seed(69)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \\\n","                                        factor=0.5, patience=20, verbose=True)\n","\n","    for epoch in range(num_epochs):\n","\n","        for i, (rgb, depth) in enumerate(train_loader):\n","\n","            if torch.cuda.is_available():\n","                rgb = rgb.cuda()\n","                depth = depth.cuda()\n","            \n","            if i % batch_multiplier == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            pred = model(rgb.float())\n","            del rgb\n","\n","            loss = torch.sqrt(criterion(depth.squeeze(), pred.squeeze())) \\\n","                    / batch_multiplier\n","            del pred, depth\n","\n","            gc.collect()\n","\n","            val_loss = batch_loss(model, nn.MSELoss(), val_loader) \\\n","                        / batch_multiplier\n","            scheduler.step(val_loss)\n","            loss.backward()\n","\n","            plotlosses.update({\n","                'loss': loss.item() * batch_multiplier,\n","                'val_loss': val_loss * batch_multiplier\n","            })\n","            plotlosses.send()\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.synchronize()\n","\n","\n","# calculate loss for all validation data (without training network)\n","# DEPRECATED, use batch_loss()\n","def validation_loss(model, criterion, val_loader, batch_size):\n","\n","    total_loss = 0.0\n","\n","    for i, (rgb, depth) in enumerate(val_loader):\n","\n","        print(\"\\rValidation Batches: {}/{}\".format(i+1, \\\n","                    int(len(val_loader.dataset)/batch_size)+1), end=\"\")\n","\n","        if torch.cuda.is_available():\n","            rgb = rgb.cuda()\n","            depth = depth.cuda()\n","        \n","        pred = model(rgb)\n","\n","        loss = torch.sqrt(criterion(depth, pred.squeeze()))\n","        #del pred\n","        total_loss += loss.item()\n","        #del loss\n","    \n","    return total_loss / len(val_loader.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpfqj415biCn","colab_type":"code","colab":{}},"source":["# stack size of vars\n","local_vars = list(locals().items())\n","for var, obj in local_vars:\n","    print(var, getsizeof(obj))"],"execution_count":null,"outputs":[]}]}